{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run.ipynb\n",
    "### This notebook lays out the extended pipeline, including all possibilities of the decision tree, where the run.py path will be extracted from. Preliminaries (Chapter 1) include:\n",
    "1. Loading in the data\n",
    "2. Creating the feature subsets\n",
    "3. Laying out the methods\n",
    "\n",
    "### After this, the pipeline (Chapter 2) can be ran where the different combinations of feature subsets, methods, and hyperparameters will be tested. The results (Chapter 3) of which will be visualized accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import inspect\n",
    "\n",
    "from validation import *\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "\n",
    "# Paths to train and test folders\n",
    "DATA_TRAIN_PATH = \"../data/train.csv\"\n",
    "DATA_TEST_PATH = \"../data/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y, train_tX, train_ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Feature subsets\n",
    "#### Including:\n",
    "- All the features as is (naive)\n",
    "- Merging highly (t = 0.96) correlated features\n",
    "\n",
    "#### Mixed with:\n",
    "- Categorical feature extraction\n",
    "- Principal Component Analysis features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_subsets = {}\n",
    "# All the features\n",
    "feature_subsets.update({\"All features\" : train_tX}) \n",
    "# Without highly correlated features\n",
    "feature_subsets.update({\"Without highly correlated features\" : \\\n",
    "                       np.delete(train_tX, [24,25,6,12,26,27,28,29], axis=1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {'least_squares' : least_squares, 'least_squares_GD' : least_squares_GD, \\\n",
    "           'least_squares_SGD' : least_squares_SGD, 'ridge_regression' : ridge_regression, \\\n",
    "           'logistic_reg_GD' : logistic_reg_GD, 'logistic_reg_newton' : logistic_reg_newton}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Helper-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Inputs a function and outputs a dictionary with the appropriate param, value pair\n",
    "\"\"\"\n",
    "def get_parameter_values(func, tX):\n",
    "    values = {}\n",
    "    mapping = {\n",
    "        'y'         : {'y'         : train_y},\n",
    "        'tx'        : {'tx'        : tX},\n",
    "        'initial_w' : {'initial_w' : np.zeros(tX.shape[1])},\n",
    "        'max_iters' : {'max_iters' : 500},\n",
    "        'gamma'     : {'gamma'     : np.logspace(-9,-7,num=10)},\n",
    "        'lambda_'   : {'lambda_'   : [0.1, 0.01, 0.001, 0.0001, 0.00001]}\n",
    "    }\n",
    "    \n",
    "    for param in inspect.signature(func).parameters:\n",
    "        values.update(mapping[param])\n",
    "        \n",
    "    return values\n",
    "\n",
    "\"\"\"\n",
    "    Returns the prediction accuracy using the prototypes and the test set\n",
    "\"\"\"\n",
    "def pred_acc(y, tx, w):\n",
    "    print(tx, w)\n",
    "    y_pred = np.squeeze(tx @ w)\n",
    "    print(y_pred)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    print(y_pred)\n",
    "    return sum(y == y_pred) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_set, tX in feature_subsets.items():\n",
    "    for method, func in methods.items():\n",
    "        parameters = get_parameter_values(func, tX)\n",
    "        \n",
    "        # assume gamma and lambda are independent\n",
    "        # hyperparameter optimization, parameters['gamma'] and ['lambda_'] hold the values\n",
    "        if 'gamma' in parameters: \n",
    "            parameters['gamma'] = 10^-8 # change\n",
    "            \n",
    "        if 'lambda_' in parameters:\n",
    "            parameters['lambda_'] = 0.0001 # change\n",
    "            \n",
    "        w, _ = func(**parameters)\n",
    "        \n",
    "        print(f\"{data_set} using {method} {pred_acc(test_y, test, w)}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
